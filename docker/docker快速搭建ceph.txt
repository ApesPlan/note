docker network create -d bridge ceph
docker network ls
docker network rm ID

docker run -itd --name mymon --network ceph -e MON_NAME=ceph_mon -e MON_IP=172.19.0.2 -v /data/ceph_mon:/etc/ceph ceph/mon




# 1. 创建Ceph专用网络 
docker network create --driver bridge --subnet 172.20.0.0/16 ceph-network

# 2. 删除旧的ceph相关容器
docker rm -f $(docker ps -a | grep ceph | awk '{print $1}')
# 3. 清理旧的ceph相关目录文件，加入有的话
rm -rf /www/ceph /var/lib/ceph/  /www/osd/
# 4. 创建相关目录及修改权限，用于挂载volume
mkdir -p /www/ceph /var/lib/ceph/osd /www/osd/
chown -R 64045:64045 /var/lib/ceph/osd/
chown -R 64045:64045 /www/osd/
# 5. 创建monitor节点
docker run -itd --name monnode --network ceph-network --ip 172.20.0.10 -e MON_NAME=monnode -e MON_IP=172.20.0.10 -v /www/ceph:/etc/ceph ceph/mon
# 6. 在monitor节点上标识3个osd节点
docker exec monnode ceph osd create
docker exec monnode ceph osd create
docker exec monnode ceph osd create
# 7. 创建OSD节点
docker run -itd --name osdnode0 --network ceph-network -e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=monnode -e MON_IP=172.20.0.10 -v /www/ceph:/etc/ceph -v /www/osd/0:/var/lib/ceph/osd/ceph-0 ceph/osd 
docker run -itd --name osdnode1 --network ceph-network -e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=monnode -e MON_IP=172.20.0.10 -v /www/ceph:/etc/ceph -v /www/osd/1:/var/lib/ceph/osd/ceph-1 ceph/osd
docker run -itd --name osdnode2 --network ceph-network -e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=monnode -e MON_IP=172.20.0.10 -v /www/ceph:/etc/ceph -v /www/osd/2:/var/lib/ceph/osd/ceph-2 ceph/osd
# 8. 增加monitor节点，组件成集群
docker run -itd --name monnode_1 --network ceph-network --ip 172.20.0.11 -e MON_NAME=monnode_1 -e MON_IP=172.20.0.11 -v /www/ceph:/etc/ceph ceph/mon
docker run -itd --name monnode_2 --network ceph-network --ip 172.20.0.12 -e MON_NAME=monnode_2 -e MON_IP=172.20.0.12 -v /www/ceph:/etc/ceph ceph/mon
# 9. 创建gateway节点
docker run -itd --name gwnode --network ceph-network --ip 172.20.0.9 -p 9080:80 -e RGW_NAME=gwnode -v /www/ceph:/etc/ceph ceph/radosgw
# 10. 查看ceph集群状态
sleep 10 && docker exec monnode ceph -s
docker exec monnode ceph -s





docker network inspect ceph-network
// 172.19.0.1
// docker run -d --name monnode --network ceph --ip 172.19.0.2 -e MON_NAME=monnode -e MON_IP=172.19.0.2 -e CEPH_PUBLIC_NETWORK=172.19.0.0/24 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /etc/localtime:/etc/localtime:ro ceph/daemon mon
docker run -d --name monnode --network ceph --ip 172.19.0.2 -e MON_NAME=monnode -e MON_IP=172.19.0.2 -e CEPH_PUBLIC_NETWORK=172.19.0.0/24 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /etc/localtime:/etc/localtime:ro ceph/daemon mon
docker run -d --name monnode1 --network ceph --ip 172.19.0.3 -e MON_NAME=monnode1 -e MON_IP=172.19.0.3 -e CEPH_PUBLIC_NETWORK=172.19.0.0/24 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /etc/localtime:/etc/localtime:ro ceph/daemon mon
docker run -d --name monnode2 --network ceph --ip 172.19.0.4 -e MON_NAME=monnode2 -e MON_IP=172.19.0.4 -e CEPH_PUBLIC_NETWORK=172.19.0.0/24 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /etc/localtime:/etc/localtime:ro ceph/daemon mon

docker run -d --name osdnode --network ceph -e OSD_TYPE=directory -e MON_NAME=monnode -e MON_IP=172.19.0.10 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /data/ceph/0:/var/lib/ceph/osd -v /etc/localtime:/etc/localtime:ro ceph/daemon osd 
docker run -d --name osdnode1 --network ceph -e OSD_TYPE=directory -e MON_NAME=monnode -e MON_IP=172.19.0.10 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /data/ceph/1:/var/lib/ceph/osd -v /etc/localtime:/etc/localtime:ro ceph/daemon osd
docker run -d --name osdnode2 --network ceph -e OSD_TYPE=directory -e MON_NAME=monnode -e MON_IP=172.19.0.10 -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /data/ceph/2:/var/lib/ceph/osd -v /etc/localtime:/etc/localtime:ro ceph/daemon osd

docker run -d --name gwnode --network ceph --ip 172.19.0.20 -p 8100:80 -e RGW_NAME=gwnode -v /data/ceph_node/ceph_mon/config:/etc/ceph -v /data/ceph_node/ceph_mon/data:/var/lib/ceph -v /etc/localtime:/etc/localtime:ro ceph/daemon rgw


[global]
fsid = e0261ad4-8670-48c9-946b-21f9141ac447
mon initial members = monnode
mon host = 172.19.0.2

auth cluster required = none
auth service required = none
auth client required = none

public network = 172.19.0.0/24
cluster network = 172.19.0.0/24
osd journal size = 100
osd_memory_target = 3052765184
osd_memory_base = 1552596992
osd_memory_cache_min = 2302681088

osd pool default size = 3
osd pool default min size = 2
osd crush update on start  = false

[mon]
    mon initial members = monnode
    mon host = 172.19.0.2
[mon.monnode]
    host = monnode
    mon addr = 172.19.0.2:8000
[client]
    rbd default format = 2


[global]  #全局，要将配置设置应用于整个集群，请在[global]下输入配置设置。
fsid = 9ec20e59-1033-4505-920d-113183167b31     #CLUSTER ID，每个Ceph存储集群都有唯一的标识符（fsid）。
mon initial members = ceph-153,ceph-154,ceph-155  #初始成员，建议最少运行三个ceph监视器在生产存储集群，以确保高可用性。启动期间集群中初始监视器的ID。 如果指定，Ceph需要奇数个监视器来形成初始定额
mon host = 192.168.1.153,192.168.1.154,192.168.1.155  #这里是主机名并非主机IP，当然如果是主机IP的形式，可以省略下面的[mon]
auth_cluster_required = cephx    #如果启用了，集群守护进程（如 ceph-mon 、 ceph-osd 和 ceph-mds ）间必须相互认证。可用选项有 cephx 或 none 。默认是cephx启用。
auth_service_required = cephx    #如果启用，客户端要访问 Ceph 服务的话，集群守护进程会要求它和集群认证。可用选项为 cephx 或 none 。类型:String是否必需:No默认值:cephx.
auth_client_required = cephx     #如果启用，客户端会要求 Ceph 集群和它认证。可用选项为 cephx 或 none 。 默认就是cephx。   
public_network = 192.168.1.0/24  #设置在[global]。可以指定以逗号分隔的子网。这是分配集群的外网网段，即对外数据交流的网段。
osd journal size = 1024    #缺省值为0。你应该使用这个参数来设置日志大小。日志大小应该至少是预期磁盘速度和filestore最大同步时间间隔的两倍。如果使用了SSD日志，最好创建大于10GB的日志，并调大filestore的最小、最大同步时间间隔。 
osd pool default size = 2   #osd的默认副本数，如果不设置默认是3份。
osd pool default min size = 1  #缺省值是0.这是处于degraded状态的副本数目，它应该小于osd pool default size的值，为存储池中的object设置最小副本数目来确认写操作。即使集群处于degraded状态。如果最小值不匹配，Ceph将不会确认写操作给客户端。 
osd pool default pg num = 333   #每个存储池默认的pg数
osd pool default pgp num = 333  #PG和PGP的个数应该保持一致。PG和PGP的值很大程度上取决于集群大小。
osd crush chooseleaf type = 1   #CRUSH规则用到chooseleaf时的bucket的类型，默认值就是1.
[osd]
osd_journal_size = 5120
[mon]
mon osd allow primary affinity = true


